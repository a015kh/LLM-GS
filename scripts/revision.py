import sys
import os
import time
import pathlib
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter

sys.path.append(".")
sys.path.append("./leaps")

from llm import LLMProgramGenerator
from prog_policies.karel import KarelDSL
from prog_policies.karel_tasks import get_task_cls
from prog_policies.search_space import get_search_space_cls
from prog_policies.search_methods import get_search_method_cls
from prog_policies.utils.save_file import (
    revision_inside_seed_save_log_file,
    revision_outside_seed_save_log_file,
)
from prog_policies.utils.evaluate_and_search import *
from prog_policies.base.task import BaseTask

if __name__ == "__main__":

    parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)

    parser.add_argument(
        "--search_space", default="ProgrammaticSpace", help="Search space class name"
    )
    parser.add_argument(
        "--search_method", default="HillClimbing", help="Search method class name"
    )
    parser.add_argument("--seed", type=int, default=1, help="Random seed for searching")
    parser.add_argument(
        "--num_iterations", type=int, default=100, help="Number of search iterations"
    )
    parser.add_argument(
        "--num_envs", type=int, default=32, help="Number of environments to search"
    )
    # StairClimberSparse, MazeSparse, FourCorners, TopOff, Harvester, CleanHouse
    # Hard: DoorKey, OneStroke, Seeder, Snake
    parser.add_argument("--task", default="StairClimber", help="Task class name")
    parser.add_argument(
        "--sigma",
        type=float,
        default=0.1,
        help="Standard deviation for Gaussian noise in Latent Space",
    )
    parser.add_argument(
        "--k", type=int, default=32, help="Number of neighbors to consider"
    )
    parser.add_argument(
        "--start_k", type=int, default=32, help="Number of neighbors to consider"
    )
    parser.add_argument(
        "--end_k", type=int, default=2048, help="Number of neighbors to consider"
    )
    parser.add_argument(
        "--e",
        type=int,
        default=8,
        help="Number of elite candidates in CEM-based methods",
    )
    parser.add_argument("--max_program_nums", type=int, default=1000000)

    # Prompt
    parser.add_argument(
        "--action_shots", type=int, default=0, help="Number of examples for action"
    )
    parser.add_argument(
        "--perception_shots",
        type=int,
        default=0,
        help="Number of examples for perception",
    )
    parser.add_argument(
        "--program_shots", type=int, default=0, help="Number of examples for program"
    )
    parser.add_argument(
        "--llm_program_num",
        type=int,
        default=32,
        help="Number of programs generated by LLM",
    )
    # LLM
    parser.add_argument("--temperature", type=float, default=1.0)
    parser.add_argument("--top_p", type=float, default=1.0)

    # Output
    parser.add_argument("--output_dir", type=str, default="output")
    parser.add_argument("--output_name", type=str, default="0")
    parser.add_argument("--save_step", type=int, default=5000)

    # Revision
    parser.add_argument("--revision_method", type=str, default="Regeneration")
    parser.add_argument("--revision_times", type=int, default=5)

    args = parser.parse_args()

    print(vars(args))

    output_dir = os.path.join(args.output_dir, args.task, args.output_name)
    output_dir_seed = os.path.join(output_dir, str(args.seed))

    pathlib.Path(output_dir_seed).mkdir(parents=True, exist_ok=True)

    dsl = KarelDSL()

    env_args = {
        "env_height": 8,
        "env_width": 8,
        "crashable": False,
        "leaps_behaviour": True,
        "max_calls": 10000,
    }

    if (
        args.task == "StairClimber"
        or args.task == "StairClimberSparse"
        or args.task == "TopOff"
        or args.task == "FourCorners"
    ):
        env_args["env_height"] = 12
        env_args["env_width"] = 12

    if args.task == "CleanHouse":
        env_args["env_height"] = 14
        env_args["env_width"] = 22

    task_cls = get_task_cls(args.task)
    task_envs = [task_cls(env_args, i) for i in range(args.num_envs)]

    search_space_cls = get_search_space_cls(args.search_space)
    search_space = search_space_cls(dsl, args.sigma)
    search_space.set_seed(args.seed)

    search_method_cls = get_search_method_cls(args.search_method)
    search_method = search_method_cls(args.k, args.e)

    best_reward = -float("inf")
    best_prog = None

    log = {}
    log["args"] = vars(args)
    log["seed"] = args.seed

    record_program_str_list = []
    record_reward_list = []

    llm_program_generator = LLMProgramGenerator(
        args.seed,
        args.task,
        dsl,
        args.llm_program_num,
        args.temperature,
        args.top_p,
        args.action_shots,
        args.perception_shots,
        args.program_shots,
    )
    program_list, llm_log = llm_program_generator.get_program_list_python_to_dsl()
    log["llm_log"] = [llm_log]

    init_time = time.time()

    # Init
    round = 0
    program_reward = []
    best_prog, best_reward, _ = record_evaluate_program_list(
        best_prog,
        best_reward,
        program_list,
        search_method,
        task_envs,
        dsl,
        program_reward,
        output_dir_seed,
        log,
        init_time,
        output_dir,
        args.task,
        args.seed,
        args.max_program_nums,
    )
    record_program_str_list.append(dsl.parse_node_to_str(best_prog))
    record_reward_list.append(best_reward)

    program_and_reward = zip(program_list, program_reward)
    program_and_reward = sorted(program_and_reward, key=lambda x: x[1], reverse=True)
    program_list = [p[0] for p in program_and_reward]

    revision_inside_seed_save_log_file(log, output_dir_seed, best_reward)
    revision_outside_seed_save_log_file(
        output_dir,
        args.task,
        args.seed,
        record_program_str_list,
        record_reward_list,
        best_reward,
    )

    # Record environment interaction
    reward_task_list = []
    for task_env in task_envs:
        reward_task_list.append((task_env, task_env.evaluate_program(best_prog)))
    reward_task_list = sorted(reward_task_list, key=lambda x: x[1])
    worst_task: BaseTask = reward_task_list[0][0]
    worst_reward, record_log = worst_task.record_evaluate_program(best_prog)

    # Revision
    for _ in range(args.revision_times):
        round += 1
        if args.revision_method == "regeneration_with_reward":
            program_list, revision_log = (
                llm_program_generator.get_program_list_revision_regeneration_with_reward(
                    [program_and_reward]
                )
            )
        elif args.revision_method == "regeneration":
            program_list, revision_log = (
                llm_program_generator.get_program_list_revision_regeneration(program_list)
            )
        elif args.revision_method == "agent_execution_trace":
            program_list, revised_log = (
                llm_program_generator.get_program_list_revision_agent_execution_trace(
                    worst_reward, record_log, best_reward
                )
            )
        elif args.revision_method == "agent_program_execution_trace":
            program_list, revised_log = (
                llm_program_generator.get_program_list_revision_agent_program_execution_trace(
                    worst_reward, record_log, best_reward
                )
            )
        else:
            raise NotImplementedError(
                f"Revision method {args.revision_method} not implemented."
            )
        log["llm_log"].append(revision_log)
        best_reward = -float("inf")
        best_prog = None
        program_reward = []
        best_prog, best_reward, _ = record_evaluate_program_list(
            best_prog,
            best_reward,
            program_list,
            search_method,
            task_envs,
            dsl,
            program_reward,
            output_dir_seed,
            log,
            init_time,
            output_dir,
            args.task,
            args.seed,
            args.max_program_nums,
        )
        record_program_str_list.append(dsl.parse_node_to_str(best_prog))
        record_reward_list.append(best_reward)
        program_and_reward = zip(program_list, program_reward)
        program_and_reward = sorted(
            program_and_reward, key=lambda x: x[1], reverse=True
        )
        program_list = [p[0] for p in program_and_reward]
        revision_inside_seed_save_log_file(log, output_dir_seed, best_reward)
        revision_outside_seed_save_log_file(
            output_dir,
            args.task,
            args.seed,
            record_program_str_list,
            record_reward_list,
            best_reward,
        )
        # Record environment interaction
        reward_task_list = []
        for task_env in task_envs:
            reward_task_list.append((task_env, task_env.evaluate_program(best_prog)))
        reward_task_list = sorted(reward_task_list, key=lambda x: x[1])
        worst_task = reward_task_list[0][0]
        worst_reward, record_log = worst_task.record_evaluate_program(best_prog)
